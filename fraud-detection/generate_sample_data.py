# -*- coding: utf-8 -*-
"""generate_sample_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OWOGfloP3ezycUaNye_z66fDKufUpath
"""

# install packages
!pip install pandas numpy matplotlib seaborn fastapi nest-asyncio pyngrok uvicorn

# import necessary pacakages
import pandas as pd
import numpy as np

data_df = pd.DataFrame()
sample_size = 5_000_000

data_df['transaction_id'] = list(range(1, sample_size))

# trans type
trans_type = [1, 2] # {1: 'DEBIT', '2':Credit}
trans_type_prob = [0.7, 0.3]
data_df['trans_type_ID'] = np.random.choice(trans_type, size=sample_size-1, p=trans_type_prob)

# transaction amount
min_amount = 0
max_amount = 1_000_000
data_df['transaction_amount'] = np.random.uniform(low=min_amount, high=max_amount, size=sample_size-1)

# trans date
start_date = pd.to_datetime('2025-01-01 00:00:00')
end_date = pd.to_datetime('2025-05-31 23:59:59')
random_integers = np.random.randint(0, int((end_date - start_date).total_seconds()), sample_size-1)
data_df['trans_date'] = start_date + pd.to_timedelta(random_integers, unit='s')

# status_type_id
status_options = [1, 2] # 1: success, 2: failed
trans_type_prob = [0.98, 0.02]
data_df['status_type_id'] = np.random.choice(trans_type, size=sample_size-1, p=trans_type_prob)

# from acc id
acc_id_min_val = 1000
acc_id_max_val = 5000
data_df['from_acc_id'] = np.random.randint(low=acc_id_min_val, high=acc_id_max_val, size=sample_size-1)


# to_acc_id
to_id_min_val = 1000
to_id_max_val = 5000
data_df['to_acc_id'] = np.random.randint(low=to_id_min_val, high=to_id_max_val, size=sample_size-1)

# is_fradulent
is_fradulent_flag = [1, 0] # 1: yes, 2: no
is_fradulent_prob = [0.4, 0.6]
data_df['is_fradulent'] = np.random.choice(is_fradulent_flag, size=sample_size-1, p=is_fradulent_prob)

data_df.shape

data_df.head()

data_df.isnull().sum()

data_df.is_fradulent.value_counts(normalize=True)

data_df.trans_type_ID.value_counts(normalize=True)

data_df.status_type_id.value_counts(normalize=True)

data_df.sort_values('trans_date').to_parquet('bank_fradulent.parquet', index=False)

# feature generation

"""Explanation of Features:

Log Transformation: Converts transaction_amount into a logarithmic scale to manage skewness and outlier effects.

Date Features: Extracts year, month, day, day of the week, and hour from tran_date to capture temporal patterns.

Interaction Features: Uses interactions between features, like multiplying transaction_amount by status_type_id, to capture combined effects.

Aggregated Features: Calculates aggregated metrics, like total transactions and average transaction amounts per account (from_acc_id).

One-Hot Encoding: Encodes categorical variables tran_type_ID and status_type_id using dummy variables.

Fraud Ratio: Computes the ratio of fraudulent transactions for each from_acc_id to capture account-level fraud tendencies.

These are just starting points. You can expand feature generation depending on the richness of your data and specific requirements, always keeping model interpretability and complexity in mind.
"""

data_df = pd.read_parquet('/content/bank_fradulent.parquet')

# Feature Engineering
feat_df = data_df.copy()

# 1. Transaction Amount Logarithm
feat_df['log_transaction_amount'] = np.log(feat_df['transaction_amount'] + 1)

# 2. Transaction Date Features (Year, Month, Day of Week, Hour)
feat_df['trans_date'] = pd.to_datetime(feat_df['trans_date'])
feat_df['trans_year'] = feat_df['trans_date'].dt.year
feat_df['trans_month'] = feat_df['trans_date'].dt.month
feat_df['trans_day'] = feat_df['trans_date'].dt.day
feat_df['trans_day_of_week'] = feat_df['trans_date'].dt.dayofweek
feat_df['trans_hour'] = feat_df['trans_date'].dt.hour

# 3. Interaction Features
feat_df['amount_status_interaction'] = feat_df['transaction_amount'] * feat_df['status_type_id']

# 4. Aggregated Features
# Total Transactions and Average Amount for each 'from_acc_id'
total_transactions_per_account = feat_df.groupby('from_acc_id').size().rename('total_transactions')
average_amount_per_account = feat_df.groupby('from_acc_id')['transaction_amount'].mean().rename('avg_amount_per_account')

feat_df = feat_df.join(total_transactions_per_account, on='from_acc_id')
feat_df = feat_df.join(average_amount_per_account, on='from_acc_id')

# 5. Encode Categorical Features
# Simple One Hot Encoding for transactional Type and Status
# feat_df = pd.get_dummies(feat_df, columns=['trans_type_ID', 'status_type_id'], prefix=['type', 'status'], drop_first=True)

# 6. Fraud Ratio of each from_acc_id
fraud_ratio = feat_df.groupby('from_acc_id')['is_fradulent'].mean().rename('fraud_ratio')

feat_df = feat_df.join(fraud_ratio, on='from_acc_id')

feat_df.to_parquet('features_feat_df.parquet', index=False)

"""# model building"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import pickle

# Assuming `df` is your DataFrame with features generated previously
feat_df = pd.read_parquet('/content/features_feat_df.parquet')

# Define features and target
skip_cols = ['transaction_id', 'transaction_amount', 'trans_date', 'from_acc_id',
       'to_acc_id', 'is_fradulent']
X = feat_df.drop(columns=skip_cols)
y = feat_df['is_fradulent']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# create standard sclaer
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# save stanard sclare model
with open('standard_scaler.pkl', 'wb') as file:
    pickle.dump(sc, file)

# Create an XGBoost Classifier
xgb_classifier = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'  # XGBoost's eval metric for binary classification
)

# Train the model
xgb_classifier.fit(X_train, y_train)

# Save using pickle
with open('xgb_model.pkl', 'wb') as file:
    pickle.dump(xgb_classifier, file)

# Make predictions
y_pred_train = xgb_classifier.predict(X_train)
y_pred_test = xgb_classifier.predict(X_test)

# Evaluate the model
print("Training Accuracy:", accuracy_score(y_train, y_pred_train))
print("Test Accuracy:", accuracy_score(y_test, y_pred_test))
print("\nClassification Report:\n", classification_report(y_test, y_pred_test))

"""# feature importance"""

import matplotlib.pyplot as plt
import xgboost as xgb

# Assuming xgb_classifier is your trained XGBoost model from previous steps

# Method 1: Extract feature importances as numbers (useful for reporting or further analysis)
importances = xgb_classifier.feature_importances_

# Print feature importances
feature_names = X.columns
importance_dict = dict(zip(feature_names, importances))
sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)

print("Feature importances:")
for feature, score in sorted_importance:
    print(f"{feature}: {score:.4f}")

# Method 2: Visualize feature importances using matplotlib plot (bar chart style)
plt.figure(figsize=(12, 8))
xgb.plot_importance(xgb_classifier, max_num_features=10, importance_type='weight')  # Choose importance_type: "weight", "gain", "cover", etc.
plt.title('Top 10 Feature Importances')
plt.show()



"""# scoring using fast-api"""

from pydantic import BaseModel, validator
from typing import List
from datetime import datetime, date, time
import pandas as pd
import numpy as np
# from fastapi import FastAPI, HTTPException
from sklearn.preprocessing import StandardScaler
import xgboost as xgb

# Define the Pydantic model for the request body
class TransData(BaseModel):
    transaction_id: int
    trans_type_ID: int
    transaction_amount: int
    trans_date: datetime
    status_type_id: int
    from_acc_id: int
    to_acc_id: int

    @validator('trans_type_ID')
    def validate_trans_type_ID(cls, value):
      trans_type_allowed_values = [1, 2]
      if value not in trans_type_allowed_values:
          raise ValueError(f"must be one of: {trans_type_allowed_values}")
      return value

    @validator('status_type_id')
    def validate_status_type_id(cls, value):
      status_type_allowed_values = [1, 2]
      if value not in status_type_allowed_values:
          raise ValueError(f"must be one of: {status_type_allowed_values}")
      return value

# feature generate

def get_histoical_trans_df(from_acc_id):
  # sql code to connect and get data
  return pd.DataFrame()

def generate_agg_feat_df(from_acc_id):
  hist_df = get_histoical_trans_df(from_acc_id)
  if not hist_df.empty:
    fraud_ratio = hist_df.groupby('from_acc_id')['is_fradulent'].mean().rename('fraud_ratio').iloc[0]
    total_transactions_per_account = hist_df.groupby('from_acc_id').size().rename('total_transactions').iloc[0]
    average_amount_per_account = hist_df.groupby('from_acc_id')['transaction_amount'].mean().rename('avg_amount_per_account').iloc[0]
  else:
    fraud_ratio = 0
    total_transactions_per_account = 0
    average_amount_per_account = 0

  return fraud_ratio, total_transactions_per_account, average_amount_per_account

def generate_features(df):

  feat_df = df.copy()

  # 0. get aggregation for from_acc_id
  fraud_ratio, total_transactions_per_account, average_amount_per_account = generate_agg_feat_df(feat_df['from_acc_id'].iloc[0])

  # 1. Transaction Amount Logarithm
  feat_df['log_transaction_amount'] = np.log(feat_df['transaction_amount'] + 1)

  # 2. Transaction Date Features (Year, Month, Day of Week, Hour)
  feat_df['trans_date'] = pd.to_datetime(feat_df['trans_date'])
  feat_df['trans_year'] = feat_df['trans_date'].dt.year
  feat_df['trans_month'] = feat_df['trans_date'].dt.month
  feat_df['trans_day'] = feat_df['trans_date'].dt.day
  feat_df['trans_day_of_week'] = feat_df['trans_date'].dt.dayofweek
  feat_df['trans_hour'] = feat_df['trans_date'].dt.hour

  # 3. Interaction Features
  feat_df['amount_status_interaction'] = feat_df['transaction_amount'] * feat_df['status_type_id']

  # 4. Aggregated Features
  # Total Transactions and Average Amount for each 'from_acc_id'
  feat_df["total_transactions"] = total_transactions_per_account
  feat_df["avg_amount_per_account"] = average_amount_per_account

  # 5. Encode Categorical Features
  # Simple One Hot Encoding for transactional Type and Status
  # feat_df = pd.get_dummies(feat_df, columns=['trans_type_ID', 'status_type_id'], prefix=['type', 'status'], drop_first=True)

  # 6. Fraud Ratio of each from_acc_id
  feat_df["fraud_ratio"] = fraud_ratio

  return feat_df

def predict_fraud(trans_data):

  skip_cols = ['transaction_id', 'transaction_amount', 'trans_date', 'from_acc_id', 'to_acc_id',]

  # converting dictionary to dataframe
  trans_df = pd.DataFrame(trans_data, index=[0])

  # generate features
  feat_df = generate_features(trans_df)
  feat_df.drop(columns=skip_cols, axis=1, inplace=True)

  # scale features
  feature_cols = list(feat_df.columns)
  scaled_feat_df = sc_model.transform(feat_df)

  # Transform features to suitable input format
  # dmatrix = xgb.DMatrix(np.float32(scaled_feat_df), feature_names=feature_cols)

  # Predict using the model
  prediction = xgboost_model.predict(scaled_feat_df)

  # Process prediction for response
  print(prediction)
  is_fraudulent = (prediction > model_threshold).astype(int)

  return is_fraudulent

# test_trans_data = {'transaction_id': 465858,
#  'trans_type_ID': 1,
#  'transaction_amount': 85951,
#  'trans_date': '2025-01-01 00:00:01',
#  'status_type_id': 1,
#  'from_acc_id': 1840,
#  'to_acc_id': 3094,
#  }

# # converting dictionary to dataframe
# test_trans_data = pd.DataFrame(test_trans_data, index=[0])

# # t_df = generate_features(test_trans_data)

# # generate features
# predict_fraud(test_trans_data)

# Initialize FastAPI app
app = FastAPI()

# Load your standard scalre
with open('/content/xgb_model.pkl', 'rb') as file:
    xgboost_model = pickle.load(file)

# Load your pre-trained model
with open('/content/standard_scaler.pkl', 'rb') as file:
    sc_model = pickle.load(file)

model_threshold = 0.6

# Define POST endpoint
@app.post("/transaction/validate/")
async def validate_transaction(trans: TransData):
  is_fraudulent = predict_fraud(trans.dict())
  return {"is_fraudulent": bool(is_fraudulent)}

import nest_asyncio
from pyngrok import ngrok
import uvicorn

ngrok_tunnel = ngrok.connect(8000)
print('Public URL:', ngrok_tunnel.public_url)
nest_asyncio.apply()
uvicorn.run(app, port=8000)

feat_df

q_df = pd.read_parquet('/content/features_feat_df.parquet')

t_df

set(t_df.columns) - set(q_df.columns)

set(q_df.columns) -set(t_df.columns)

y_test_pred



q_df.drop([skip_cols])

skip_cols = ['transaction_id',
 'transaction_amount',
 'trans_date',
 'from_acc_id',
 'to_acc_id',
 'is_fradulent']

set(xgb_classifier.predict(sc.transform(q_df.drop(skip_cols, axis=1))))

